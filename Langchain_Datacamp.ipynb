{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d53e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "huggingfacehub_api_token = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fca12cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader, HNLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_openai import OpenAI, ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127eb405",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d81cfede-b7e8-406c-ac28-fff20ea99df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whatever you do, take care of your shoes.\n",
      "I'm a big fan of the \"take care of your shoes\" philosophy. I've had a lot of shoes in my life, and I've learned that if you take care of them, they will take care of you.\n",
      "I've had a lot of shoes in my life, and I've learned that if you take care of them, they will take care of you.\n",
      "I've had a lot of shoes in my life, and I\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# Define the LLM\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bae850a-adb8-4fea-8853-cb948054bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Shoes are an essential part of our daily lives, providing protection and support for our feet as we go about our activities. They also play a major role in our overall appearance and can make or break an outfit. That's why it's important to take proper care of your shoes. Here are some tips to help you keep your shoes in top condition:\n",
      "\n",
      "1. Clean them regularly: The first step to taking care of your shoes is to keep them clean. Wipe them down with a damp cloth after each use to remove any dirt or debris. For tougher stains, use a shoe cleaner specifically designed for the material of your shoes.\n",
      "\n",
      "2. Store them properly: When you're not wearing your shoes, make sure to store them in a cool, dry place. Avoid keeping them in direct sunlight or near sources of heat, as this can cause the material to crack or fade.\n",
      "\n",
      "3. Use shoe trees: To maintain the shape of your shoes, invest in a pair of shoe trees. These will help prevent creasing and maintain the structure of your shoes.\n",
      "\n",
      "4. Rotate your shoes: Wearing the same pair of shoes every day can cause them to wear out quickly. To prolong their lifespan, rotate your shoes and give them a break in between wears.\n",
      "\n",
      "5.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# Define the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "# Predict the words following the text in question\n",
    "question = 'Whatever you do, take care of your shoes'\n",
    "output = llm.invoke(question)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57699982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an artificial intelligence assistant, answer the question. How does LangChain make LLM application development easier?\n",
      "LangChain provides a platform for LLM application development that simplifies the process by offering a range of tools and services. It allows developers to focus on the core of their application while handling the low-level details such as database management, authentication, and authorization. LangChain also offers a variety of pre-built modules and components that can be easily integrated into LLM applications, reducing development time and cost. Additionally, LangChain provides a secure and scalable infrastructure that can handle large volumes of traffic and data\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt template from the template string\n",
    "template = \"You are an artificial intelligence assistant, answer the question. {question}\"\n",
    "prompt = PromptTemplate(template=template, input_variables=['question'])\n",
    "\n",
    "# Create a chain to integrate the prompt template and LLM\n",
    "llm = HuggingFaceHub(repo_id='tiiuae/falcon-7b-instruct', huggingfacehub_api_token=huggingfacehub_api_token)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"How does LangChain make LLM application development easier?\"\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46f2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an OpenAI chat model\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\t\t\n",
    "\n",
    "# Create a chat prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"Respond to question: {question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Insert a question into the template and call the model\n",
    "full_prompt = prompt_template.format_messages(question='How can I retain learning?')\n",
    "llm(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58e11a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"A list comprehension is a concise way to create lists in Python. It allows you to create a new list by applying an expression to each item in an existing iterable (such as a list, tuple, or range) and filtering the items based on a condition.\\n\\nHere's a simple example of a list comprehension that creates a list of squared numbers from 0 to 9:\\n\\n```python\\nsquared_numbers = [x**2 for x in range(10)]\\nprint(squared_numbers)\\n```\\n\\nThis will output:\\n\\n```\\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\\n```\\n\\nList comprehensions are a powerful and concise way to work with lists in Python and are often preferred over traditional for loops for their readability and simplicity.\"\n",
      "content='List comprehension is a concise way to create lists in Python by applying an expression to each item in an iterable.'\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the conversation history and add the first AI message\n",
    "history = ChatMessageHistory()\n",
    "history.add_ai_message(\"Hello! Ask me anything about Python programming!\")\n",
    "\n",
    "# Add the user message to the history and call the model\n",
    "history.add_user_message(\"What is a list comprehension?\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)\n",
    "\n",
    "# Add another user message and call the model\n",
    "history.add_user_message(\"Describe the same in fewer words\")\n",
    "ai_response = chat(history.messages)\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "567e2c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Write Python code to draw a scatter plot.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Write Python code to draw a scatter plot.\n",
      "AI:  Sure, I can definitely help you with that! To draw a scatter plot in Python, you will need to import the matplotlib library. This can be done by using the command \"import matplotlib.pyplot as plt\". Then, you can create a scatter plot by using the \"scatter()\" function and passing in the x and y values as parameters. For example, if you have two lists of data called \"x_data\" and \"y_data\", you can create a scatter plot by using the code \"plt.scatter(x_data, y_data)\". You can also customize your scatter plot by adding a title, labels for the x and y axes, and changing the color and size of the data points. Is there anything else you would like to know about drawing a scatter plot in Python?\n",
      "Human: Use the Seaborn library.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Ah, great question! Seaborn is another popular library for data visualization in Python. To use Seaborn to draw a scatter plot, you will first need to import it using the command \"import seaborn as sns\". Then, you can use the \"scatterplot()\" function and pass in the x and y values as well as the name of the data frame containing your data. For example, if your data frame is called \"df\" and you want to plot the columns \"x\" and \"y\", you can use the code \"sns.scatterplot(x=\\'x\\', y=\\'y\\', data=df)\". Seaborn also offers many customization options for your scatter plot, such as adding a trendline or changing the shape and color of the data points. Is there anything else you would like to know about using Seaborn for scatter plots?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a buffer memory\n",
    "memory = ConversationBufferMemory(size=4)\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "buffer_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "buffer_chain.predict(input=\"Write Python code to draw a scatter plot.\")\n",
    "buffer_chain.predict(input=\"Use the Seaborn library.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0d1c388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Describe the relationship of the human mind with the keyboard when taking a great online class.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "The human and AI discuss the relationship between the human mind and the keyboard when taking a great online class. They conclude that the keyboard acts as a bridge between the two, facilitating collaboration and communication. It allows for the human mind to input information and interact with the class, while also expressing thoughts and ideas in a written format. This enhances the learning experience for the human.\n",
      "Human: Use an analogy to describe it.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Hmm, let me think. I would say the relationship between the human mind and the keyboard in an online class is like a conductor and their baton in an orchestra. The conductor uses the baton to guide and direct the musicians, just as the keyboard allows the human mind to guide and interact with the online class. Both are essential tools for creating a harmonious and successful performance or learning experience.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "# Define a summary memory that uses an OpenAI model\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key))\n",
    "\n",
    "# Define the chain for integrating the memory with the model\n",
    "summary_chain = ConversationChain(llm=chat, memory=memory, verbose=True)\n",
    "\n",
    "# Invoke the chain with the inputs provided\n",
    "summary_chain.predict(input=\"Describe the relationship of the human mind with the keyboard when taking a great online class.\")\n",
    "summary_chain.predict(input=\"Use an analogy to describe it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cd1f3",
   "metadata": {},
   "source": [
    "## Loading and Preparing External Data For Chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3ae1994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.comNoam Shazeer∗\\nGoogle Brain\\nnoam@google.comNiki Parmar∗\\nGoogle Research\\nnikip@google.comJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.comAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.eduŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023' metadata={'source': 'learning-langchain/attention-is-all-you-need.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create a document loader for attention_is_all_you_need.pdf\n",
    "loader = PyPDFLoader('learning-langchain/attention-is-all-you-need.pdf')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f757be4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='country: United States\\nconfederation: CONCACAF\\npopulation_share: 4.5\\ntv_audience_share: 4.3\\ngdp_weighted_share: 11.3' metadata={'source': 'learning-langchain/fifa_countries_audience.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "# Create a document loader for fifa_countries_audience.csv\n",
    "loader = CSVLoader('learning-langchain/fifa_countries_audience.csv')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0014aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Nvtop: Linux Task Monitor for Nvidia, AMD and Intel GPUs (github.com/syllo)' metadata={'source': 'https://news.ycombinator.com', 'title': 'Nvtop: Linux Task Monitor for Nvidia, AMD and Intel GPUs (github.com/syllo)', 'link': 'https://github.com/Syllo/nvtop', 'ranking': '1.'}\n",
      "{'source': 'https://news.ycombinator.com', 'title': 'Nvtop: Linux Task Monitor for Nvidia, AMD and Intel GPUs (github.com/syllo)', 'link': 'https://github.com/Syllo/nvtop', 'ranking': '1.'}\n"
     ]
    }
   ],
   "source": [
    "# Create a document loader for the top Hacker News stories\n",
    "loader = HNLoader('https://news.ycombinator.com')\n",
    "\n",
    "# Load the document\n",
    "data = loader.load()\n",
    "\n",
    "# Print the first document\n",
    "print(data[0])\n",
    "\n",
    "# Print the first document's metadata\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53d2b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.']\n"
     ]
    }
   ],
   "source": [
    "quote = 'One machine can do the work of fifty ordinary humans. No machine can do the work of one extraordinary human.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 3\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = CharacterTextSplitter(chunk_size=24, chunk_overlap=3)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f1f678f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words are flowing out', 'out like endless rain', 'rain into a paper cup,', 'they slither while they', 'they pass,', 'they slip away across', 'across the universe.']\n"
     ]
    }
   ],
   "source": [
    "quote = 'Words are flowing out like endless rain into a paper cup,\\nthey slither while they pass,\\nthey slip away across the universe.'\n",
    "chunk_size = 24\n",
    "chunk_overlap = 10\n",
    "\n",
    "# Create an instance of the splitter class\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Split the document and print the chunks\n",
    "docs = splitter.split_text(quote)\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34bac07e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'hnswlib.Index' has no attribute 'file_handle_count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m embedding_model \u001b[39m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[39m=\u001b[39mopenai_api_key)\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Create the Chroma vector DB using the OpenAI embedding function; persist the database\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m vectordb \u001b[39m=\u001b[39m Chroma(\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     persist_directory\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39membedding/chroma/\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     embedding_function\u001b[39m=\u001b[39;49membedding_model\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://vscode-01hr23wrgapedmfykr5ka7w7fv.studio.lightning.ai/teamspace/studios/this_studio/learning-langchain/Langchain_Datacamp.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m vectordb\u001b[39m.\u001b[39mpersist()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:120\u001b[0m, in \u001b[0;36mChroma.__init__\u001b[0;34m(self, collection_name, embedding_function, persist_directory, client_settings, collection_metadata, client, relevance_score_fn)\u001b[0m\n\u001b[1;32m    118\u001b[0m         _client_settings \u001b[39m=\u001b[39m chromadb\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mSettings()\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client_settings \u001b[39m=\u001b[39m _client_settings\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client \u001b[39m=\u001b[39m chromadb\u001b[39m.\u001b[39;49mClient(_client_settings)\n\u001b[1;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persist_directory \u001b[39m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m         _client_settings\u001b[39m.\u001b[39mpersist_directory \u001b[39mor\u001b[39;00m persist_directory\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function \u001b[39m=\u001b[39m embedding_function\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/__init__.py:274\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(settings, tenant, database)\u001b[0m\n\u001b[1;32m    271\u001b[0m tenant \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(tenant)\n\u001b[1;32m    272\u001b[0m database \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(database)\n\u001b[0;32m--> 274\u001b[0m \u001b[39mreturn\u001b[39;00m ClientCreator(tenant\u001b[39m=\u001b[39;49mtenant, database\u001b[39m=\u001b[39;49mdatabase, settings\u001b[39m=\u001b[39;49msettings)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/api/client.py:139\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, tenant, database, settings)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    135\u001b[0m     tenant: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m DEFAULT_TENANT,\n\u001b[1;32m    136\u001b[0m     database: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m DEFAULT_DATABASE,\n\u001b[1;32m    137\u001b[0m     settings: Settings \u001b[39m=\u001b[39m Settings(),\n\u001b[1;32m    138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(settings\u001b[39m=\u001b[39;49msettings)\n\u001b[1;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtenant \u001b[39m=\u001b[39m tenant\n\u001b[1;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatabase \u001b[39m=\u001b[39m database\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/api/client.py:43\u001b[0m, in \u001b[0;36mSharedSystemClient.__init__\u001b[0;34m(self, settings)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     39\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     40\u001b[0m     settings: Settings \u001b[39m=\u001b[39m Settings(),\n\u001b[1;32m     41\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_identifier \u001b[39m=\u001b[39m SharedSystemClient\u001b[39m.\u001b[39m_get_identifier_from_settings(settings)\n\u001b[0;32m---> 43\u001b[0m     SharedSystemClient\u001b[39m.\u001b[39;49m_create_system_if_not_exists(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_identifier, settings)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/api/client.py:54\u001b[0m, in \u001b[0;36mSharedSystemClient._create_system_if_not_exists\u001b[0;34m(cls, identifier, settings)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_identifer_to_system[identifier] \u001b[39m=\u001b[39m new_system\n\u001b[1;32m     53\u001b[0m     new_system\u001b[39m.\u001b[39minstance(ProductTelemetryClient)\n\u001b[0;32m---> 54\u001b[0m     new_system\u001b[39m.\u001b[39;49minstance(ServerAPI)\n\u001b[1;32m     56\u001b[0m     new_system\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     57\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/config.py:382\u001b[0m, in \u001b[0;36mSystem.instance\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m get_class(fqn, \u001b[39mtype\u001b[39m)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_instances:\n\u001b[0;32m--> 382\u001b[0m     impl \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_instances[\u001b[39mtype\u001b[39m] \u001b[39m=\u001b[39m impl\n\u001b[1;32m    384\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/api/segment.py:103\u001b[0m, in \u001b[0;36mSegmentAPI.__init__\u001b[0;34m(self, system)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_settings \u001b[39m=\u001b[39m system\u001b[39m.\u001b[39msettings\n\u001b[1;32m    102\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sysdb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire(SysDB)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_manager \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequire(SegmentManager)\n\u001b[1;32m    104\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_quota \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire(QuotaEnforcer)\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_product_telemetry_client \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequire(ProductTelemetryClient)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/config.py:281\u001b[0m, in \u001b[0;36mComponent.require\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequire\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mtype\u001b[39m: Type[T]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m    279\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a Component instance of the given type, and register as a dependency of\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[39m    that instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 281\u001b[0m     inst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_system\u001b[39m.\u001b[39;49minstance(\u001b[39mtype\u001b[39;49m)\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dependencies\u001b[39m.\u001b[39madd(inst)\n\u001b[1;32m    283\u001b[0m     \u001b[39mreturn\u001b[39;00m inst\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/config.py:382\u001b[0m, in \u001b[0;36mSystem.instance\u001b[0;34m(self, type)\u001b[0m\n\u001b[1;32m    379\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m get_class(fqn, \u001b[39mtype\u001b[39m)\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_instances:\n\u001b[0;32m--> 382\u001b[0m     impl \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39;49m(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    383\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_instances[\u001b[39mtype\u001b[39m] \u001b[39m=\u001b[39m impl\n\u001b[1;32m    384\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_running:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/segment/impl/manager/local.py:85\u001b[0m, in \u001b[0;36mLocalSegmentManager.__init__\u001b[0;34m(self, system)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_file_handles \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mwindll\u001b[39m.\u001b[39mmsvcrt\u001b[39m.\u001b[39m_getmaxstdio()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     83\u001b[0m segment_limit \u001b[39m=\u001b[39m (\n\u001b[1;32m     84\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_file_handles\n\u001b[0;32m---> 85\u001b[0m         \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m PersistentLocalHnswSegment\u001b[39m.\u001b[39;49mget_file_handle_count()\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_instances_file_handle_cache \u001b[39m=\u001b[39m LRUCache(\n\u001b[1;32m     88\u001b[0m     segment_limit, callback\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m _, v: v\u001b[39m.\u001b[39mclose_persistent_index()\n\u001b[1;32m     89\u001b[0m )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/chromadb/segment/impl/vector/local_persistent_hnsw.py:445\u001b[0m, in \u001b[0;36mPersistentLocalHnswSegment.get_file_handle_count\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    443\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_file_handle_count\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    444\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return how many file handles are used by the index\"\"\"\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m     hnswlib_count \u001b[39m=\u001b[39m hnswlib\u001b[39m.\u001b[39;49mIndex\u001b[39m.\u001b[39;49mfile_handle_count\n\u001b[1;32m    446\u001b[0m     hnswlib_count \u001b[39m=\u001b[39m cast(\u001b[39mint\u001b[39m, hnswlib_count)\n\u001b[1;32m    447\u001b[0m     \u001b[39m# One extra for the metadata file\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'hnswlib.Index' has no attribute 'file_handle_count'"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader('learning-langchain/attention-is-all-you-need.pdf')\n",
    "data = loader.load()\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "# Define an OpenAI embeddings model\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "\n",
    "# Create the Chroma vector DB using the OpenAI embedding function; persist the database\n",
    "vectordb = Chroma(\n",
    "    persist_directory='embedding/chroma/', \n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab68926",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
